{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Analise exploratória"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import sys\n",
    "\n",
    "parent = Path().absolute().parents[0].as_posix()\n",
    "\n",
    "sys.path.insert(0, parent)\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "import pandas as pdss\n",
    "import numpy as np\n",
    "\n",
    "import yake\n",
    "import spacy\n",
    "\n",
    "#from spacytextblob.spacytextblob import SpacyTextBlob\n",
    "\n",
    "from nlpiper.core import Compose\n",
    "from nlpiper.transformers import cleaners\n",
    "from nlpiper.core import Document\n",
    "\n",
    "\n",
    "from gensim.corpora.dictionary import Dictionary\n",
    "from gensim import models \n",
    "\n",
    "from sklearn.cluster import DBSCAN, KMeans\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "from resources.stopwords import WORDS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spacy.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load(\"pt_core_news_lg\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for word in nlp('Esta é uma fila.'):\n",
    "    print(word.pos_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv('../data/scraping_data.csv.gz', compression='gzip')\n",
    "data_political_parties = pd.read_csv('../data/scraping_political_parties.csv.gz', compression='gzip')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.isna().any()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.dropna(inplace=True)\n",
    "data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_political_parties.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.city.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.groupby(['year', 'city']).size()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Pre-processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline = Compose([\n",
    "    cleaners.CleanURL(),\n",
    "    #cleaners.CleanPunctuation(),\n",
    "    cleaners.CleanEOF(),\n",
    "    cleaners.CleanMarkup(),\n",
    "    cleaners.CleanAccents(),\n",
    "    cleaners.CleanNumber(),\n",
    "    #tokenizers.BasicTokenizer()\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "simple_pipeline = Compose([\n",
    "    cleaners.CleanURL(),\n",
    "    cleaners.CleanEOF(),\n",
    "    cleaners.CleanMarkup(),\n",
    "    cleaners.CleanAccents(),\n",
    "    #cleaners.CleanNumber(),\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_words = Document(WORDS)\n",
    "stop_words_ = pipeline(stop_words)\n",
    "stop_words_ = stop_words_.cleaned.split(' ')\n",
    "stop_words_ = list(filter(None, stop_words_))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "class TextCleaner:\n",
    "    \n",
    "    def __init__(self, model, stop_words):\n",
    "        self.model = model\n",
    "        self.stop_words = stop_words\n",
    "        \n",
    "    def __call__(self, document):\n",
    "        \n",
    "        processed_doc = self._apply_pos_tagger(document)\n",
    "        processed_doc = self._remove_punctuation(processed_doc)\n",
    "        processed_doc = self._remove_stop_words(processed_doc)\n",
    "        processed_doc = self._remove_double_spaces(processed_doc)\n",
    "        processed_doc = self._remove_space_at_sentence_end(processed_doc)\n",
    "        \n",
    "        return processed_doc\n",
    "\n",
    "    def _apply_pos_tagger(self, document):\n",
    "\n",
    "        tokens = []\n",
    "        for word in self.model(document):\n",
    "            if word.pos_ in ['NOUN', 'ADJ', 'VERB', 'ADV', 'PUNCT']:\n",
    "                tokens.append(word.text)\n",
    "        return ' '.join(tokens)\n",
    "\n",
    "    def _remove_stop_words(self, document):\n",
    "        return ' '.join([word for word in document.split(' ') if word not in self.stop_words])        \n",
    "\n",
    "    def _remove_double_spaces(self, document):\n",
    "        return document.replace('  ', ' ')\n",
    "\n",
    "    def _remove_punctuation(self, document):\n",
    "        punctuation =  '!\"#$%&\\'()*+,-/:;<=>?@[\\\\]”^“_`{|}~'\n",
    "        return document.translate(str.maketrans('', '', punctuation))\n",
    "\n",
    "    def _remove_space_at_sentence_end(self, document):\n",
    "        return document.replace(' .', '.')    \n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tc = TextCleaner(model=nlp, stop_words=stop_words_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "docs = []\n",
    "docs_on_tokens = []\n",
    "for _, val in tqdm(data.sample(n=100, random_state=1).iterrows()):\n",
    "    doc = Document(val['content'].lower())\n",
    "    doc_p = pipeline(doc)\n",
    "    doc_p = tc(doc_p.cleaned)\n",
    "    docs.append(doc_p)\n",
    "    docs_on_tokens.append(doc_p.split(' '))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "docs_on_tokens = np.load('../data/processed/docs_cleaned.npz', allow_pickle=True)['files']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(docs_on_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "docs_on_tokens[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.content.iloc[2].lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "docs[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "re.sub(' .', '.', docs[2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Topic modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a corpus from a list of texts\n",
    "dictionary = Dictionary(docs_on_tokens)\n",
    "corpus = [dictionary.doc2bow(text) for text in docs_on_tokens]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dictionary.token2id.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = models.LdaModel(corpus, num_topics=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(model.print_topics())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyLDAvis\n",
    "import pyLDAvis.gensim_models as gensimvis\n",
    "\n",
    "pyLDAvis.enable_notebook()\n",
    "gensimvis.prepare(model, corpus, dictionary)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Keyword Detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.link.iloc[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "language = \"pt\"\n",
    "max_ngram_size = 3\n",
    "deduplication_thresold = 0.9\n",
    "deduplication_algo = 'seqm'\n",
    "windowSize = 1\n",
    "numOfKeywords = 5\n",
    "\n",
    "custom_kw_extractor = yake.KeywordExtractor(\n",
    "    lan=language, \n",
    "    n=max_ngram_size, \n",
    "    dedupLim=deduplication_thresold, \n",
    "    dedupFunc=deduplication_algo, \n",
    "    windowsSize=windowSize, \n",
    "    top=numOfKeywords, \n",
    "    features=None\n",
    ")\n",
    "keywords = custom_kw_extractor.extract_keywords(docs[0])\n",
    "\n",
    "for kw in keywords:\n",
    "    print(kw)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "keywords_on_docs = []\n",
    "for idx in range(len(data)):\n",
    "    keywords_on_docs.append(custom_kw_extractor.extract_keywords(simple_pipeline(Document(data.content.iloc[idx].lower())).cleaned))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "custom_kw_extractor.extract_keywords(simple_pipeline(Document(data.content.iloc[0].lower())).cleaned)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.test.utils import common_texts\n",
    "from gensim.models import Word2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Word2Vec(sentences=docs_on_tokens, vector_size=100, window=5, min_count=1, workers=4, sg=0, epochs=30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.wv.most_similar('rei', topn=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.wv.similarity('homen', 'rei')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "keywords_embeddings = []\n",
    "for doc in keywords_on_docs:\n",
    "    for word, _ in doc:\n",
    "        try:\n",
    "            keywords_embeddings.append(model.wv.get_vector(word))\n",
    "        except:\n",
    "            continue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embs_on_docs = []\n",
    "word_corpora = []\n",
    "for doc in tqdm(docs_on_tokens[0:]):\n",
    "    for word in doc:\n",
    "        if word not in word_corpora:\n",
    "            embs_on_docs.append((word, model.wv.get_vector(word)))\n",
    "            word_corpora.append(word)\n",
    "        else:\n",
    "            continue\n",
    "        \n",
    "np.savez_compressed('../data/processed/docs_embbeded', files=embs_on_docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(embs_on_docs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = [' '.join(doc) for doc in docs_on_tokens]\n",
    "vectorizer = TfidfVectorizer()\n",
    "X = vectorizer.fit_transform(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = vectorizer.transform(['covid ataca cada vez mais.'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(x.toarray()[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w = dict(zip(vectorizer.get_feature_names(), x.toarray()[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w['covid']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import DBSCAN, KMeans, MiniBatchKMeans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clustering = DBSCAN(eps=3, min_samples=2).fit(keywords_embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(np.unique(clustering.labels_))  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kmeans_preds = KMeans(n_clusters=20, random_state=0).fit([val[1] for val in embs_on_docs])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kmeans_preds.labels_ "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res = pd.DataFrame(columns=['Word', 'Concept'])\n",
    "res['Word'] = [val[0] for val in embs_on_docs]\n",
    "res['Concept'] = kmeans_preds.labels_ "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res.Concept.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res[res.Concept == 15].head(50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sentiment Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "5ced3be9d1c63e918c342e50044be0791acd9298dca62153cfbef56e0b5c2b15"
  },
  "kernelspec": {
   "display_name": "Python 3.9.12 ('smart_archive-_uPZh7PR-py3.9')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
