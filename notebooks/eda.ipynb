{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Analise exploratória"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import sys\n",
    "\n",
    "parent = Path().absolute().parents[0].as_posix()\n",
    "\n",
    "sys.path.insert(0, parent)\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import yake\n",
    "import spacy\n",
    "import pickle\n",
    "\n",
    "from nlpiper.core import Compose\n",
    "from nlpiper.transformers import cleaners\n",
    "from nlpiper.core import Document\n",
    "\n",
    "\n",
    "from gensim.corpora.dictionary import Dictionary\n",
    "from gensim import models \n",
    "\n",
    "from sklearn.cluster import DBSCAN, KMeans\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "\n",
    "from resources.stopwords import WORDS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'3.2.4'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spacy.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load(\"pt_core_news_lg\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for word in nlp('Esta é uma fila.'):\n",
    "    print(word.pos_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv('../data/scraping_data.csv.gz', compression='gzip')\n",
    "data_v2 = pd.read_csv('../data/scraping_data_v2.csv.gz', compression='gzip')\n",
    "data_political_parties = pd.read_csv('../data/scraping_political_parties.csv.gz', compression='gzip')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>city</th>\n",
       "      <th>title</th>\n",
       "      <th>content</th>\n",
       "      <th>year</th>\n",
       "      <th>tstamp</th>\n",
       "      <th>link</th>\n",
       "      <th>source</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>Lisboa</td>\n",
       "      <td>Expresso | PSP de Lisboa detém falso polícia p...</td>\n",
       "      <td>Expresso | PSP de Lisboa detém falso polícia p...</td>\n",
       "      <td>2019</td>\n",
       "      <td>20191014194634</td>\n",
       "      <td>https://arquivo.pt/wayback/20191014194634/http...</td>\n",
       "      <td>expresso.pt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>Lisboa</td>\n",
       "      <td>Lisboa | Lisboa | PÚBLICO</td>\n",
       "      <td>Lisboa | Lisboa | PÚBLICO Ir para o conteúdo I...</td>\n",
       "      <td>2019</td>\n",
       "      <td>20190831124050</td>\n",
       "      <td>https://arquivo.pt/wayback/20190831124050/http...</td>\n",
       "      <td>publico.pt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>Lisboa</td>\n",
       "      <td>Treze pessoas detidas por furtos qualificados ...</td>\n",
       "      <td>Treze pessoas detidas por furtos qualificados ...</td>\n",
       "      <td>2019</td>\n",
       "      <td>20190522144307</td>\n",
       "      <td>https://arquivo.pt/wayback/20190522144307/http...</td>\n",
       "      <td>observador.pt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>Porto</td>\n",
       "      <td>Expresso | Porto</td>\n",
       "      <td>Expresso | Porto Assinar Loja Siga-nos Faceboo...</td>\n",
       "      <td>2019</td>\n",
       "      <td>20190522154115</td>\n",
       "      <td>https://arquivo.pt/wayback/20190522154115/http...</td>\n",
       "      <td>expresso.pt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>Porto</td>\n",
       "      <td>Porto | Porto | PÚBLICO</td>\n",
       "      <td>Porto | Porto | PÚBLICO Ir para o conteúdo Ir ...</td>\n",
       "      <td>2019</td>\n",
       "      <td>20190831124749</td>\n",
       "      <td>https://arquivo.pt/wayback/20190831124749/http...</td>\n",
       "      <td>publico.pt</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0    city                                              title  \\\n",
       "0           0  Lisboa  Expresso | PSP de Lisboa detém falso polícia p...   \n",
       "1           1  Lisboa                          Lisboa | Lisboa | PÚBLICO   \n",
       "2           2  Lisboa  Treze pessoas detidas por furtos qualificados ...   \n",
       "3           3   Porto                                   Expresso | Porto   \n",
       "4           4   Porto                            Porto | Porto | PÚBLICO   \n",
       "\n",
       "                                             content  year          tstamp  \\\n",
       "0  Expresso | PSP de Lisboa detém falso polícia p...  2019  20191014194634   \n",
       "1  Lisboa | Lisboa | PÚBLICO Ir para o conteúdo I...  2019  20190831124050   \n",
       "2  Treze pessoas detidas por furtos qualificados ...  2019  20190522144307   \n",
       "3  Expresso | Porto Assinar Loja Siga-nos Faceboo...  2019  20190522154115   \n",
       "4  Porto | Porto | PÚBLICO Ir para o conteúdo Ir ...  2019  20190831124749   \n",
       "\n",
       "                                                link         source  \n",
       "0  https://arquivo.pt/wayback/20191014194634/http...    expresso.pt  \n",
       "1  https://arquivo.pt/wayback/20190831124050/http...     publico.pt  \n",
       "2  https://arquivo.pt/wayback/20190522144307/http...  observador.pt  \n",
       "3  https://arquivo.pt/wayback/20190522154115/http...    expresso.pt  \n",
       "4  https://arquivo.pt/wayback/20190831124749/http...     publico.pt  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_v2.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.isna().any()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.dropna(inplace=True)\n",
    "data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_political_parties.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.city.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.groupby(['year', 'city']).size()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Pre-processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#docs_on_tokens = np.load('../data/processed/docs_cleaned.npz', allow_pickle=True)['files']\n",
    "import pickle\n",
    "with open('../data/processed/docs_cleaned.pickle', 'rb') as handle:\n",
    "    docs_on_tokens = pickle.load(handle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "30712"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(docs_on_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "top_n = 50\n",
    "vectorizer = TfidfVectorizer()\n",
    "X = vectorizer.fit_transform(docs_on_tokens[0:1000])\n",
    "feature_array = vectorizer.get_feature_names()\n",
    "\n",
    "tf_idf_scores = sorted(list(zip(vectorizer.get_feature_names(), \n",
    "                                             X.sum(0).getA1())), \n",
    "                                 key=lambda x: x[1], reverse=True)\n",
    "\n",
    "print('tf_idf scores: \\n', tf_idf_scores[:top_n])\n",
    "\n",
    "\n",
    "print('idf values: \\n', sorted(list(zip(feature_array,vectorizer.idf_,)),\n",
    "       key = lambda x: x[1], reverse=True)[:top_n])\n",
    "\n",
    "vectorizer = CountVectorizer()\n",
    "X = vectorizer.fit_transform(docs_on_tokens[0:top_n])\n",
    "feature_array = vectorizer.get_feature_names()\n",
    "print('Frequency: \\n', sorted(list(zip(vectorizer.get_feature_names(), \n",
    "                                         X.sum(0).getA1())),\n",
    "                            key=lambda x: x[1], reverse=True)[:top_n])       \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf_idf_scores[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(X.indices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(X.data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "corpus = [\"This is very strange\",\n",
    "          \"This is very nice\"]\n",
    "vectorizer = TfidfVectorizer(norm='l2')\n",
    "corpus = vectorizer.fit_transform(corpus)\n",
    "\n",
    "print(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer.vocabulary_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(dict(zip(vectorizer.vocabulary_, vectorizer.idf_)))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Topic modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a corpus from a list of texts\n",
    "dictionary = Dictionary(docs_on_tokens)\n",
    "corpus = [dictionary.doc2bow(text) for text in docs_on_tokens]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dictionary.token2id.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = models.LdaModel(corpus, num_topics=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(model.print_topics())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyLDAvis\n",
    "import pyLDAvis.gensim_models as gensimvis\n",
    "\n",
    "pyLDAvis.enable_notebook()\n",
    "gensimvis.prepare(model, corpus, dictionary)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Keyword Detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.link.iloc[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "language = \"pt\"\n",
    "max_ngram_size = 3\n",
    "deduplication_thresold = 0.9\n",
    "deduplication_algo = 'seqm'\n",
    "windowSize = 1\n",
    "numOfKeywords = 5\n",
    "\n",
    "custom_kw_extractor = yake.KeywordExtractor(\n",
    "    lan=language, \n",
    "    n=max_ngram_size, \n",
    "    dedupLim=deduplication_thresold, \n",
    "    dedupFunc=deduplication_algo, \n",
    "    windowsSize=windowSize, \n",
    "    top=numOfKeywords, \n",
    "    features=None\n",
    ")\n",
    "keywords = custom_kw_extractor.extract_keywords(docs[0])\n",
    "\n",
    "for kw in keywords:\n",
    "    print(kw)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "keywords_on_docs = []\n",
    "for idx in range(len(data)):\n",
    "    keywords_on_docs.append(custom_kw_extractor.extract_keywords(simple_pipeline(Document(data.content.iloc[idx].lower())).cleaned))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "custom_kw_extractor.extract_keywords(simple_pipeline(Document(data.content.iloc[0].lower())).cleaned)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('../data/processed/docs_keywords.pickle', 'rb') as handle:\n",
    "        docs_keywords = pickle.load(handle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('arbitros recusaram dirigir', '__label__Negative'),\n",
       " ('recusaram dirigir jogo', '__label__Negative'),\n",
       " ('dirigir jogo beiramar', '__label__Negative'),\n",
       " ('recusaram dirigir', '__label__Negative'),\n",
       " ('arbitros recusaram', '__label__Negative'),\n",
       " ('jogo beiramar suspensos.', '__label__Positive'),\n",
       " ('beiramar sporting suspensos', '__label__Positive'),\n",
       " ('dirigir jogo', '__label__Positive'),\n",
       " ('jogo beiramar sporting', '__label__Positive'),\n",
       " ('recusaram dirigir partida', '__label__Negative')]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t = []\n",
    "for keywords in docs_keywords[-50]:\n",
    "        t.append((keywords[0], model.predict(keywords[0])[0][0]))\n",
    "t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Counter({'__label__Negative': 6, '__label__Positive': 4})"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from collections import Counter\n",
    "Counter([v[1] for v in t])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'__label__Positive'"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.predict(docs_keywords[50][-2][0])[0][0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.test.utils import common_texts\n",
    "from gensim.models import Word2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Word2Vec(sentences=docs_on_tokens, vector_size=100, window=5, min_count=1, workers=4, sg=0, epochs=30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.wv.most_similar('rei', topn=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.wv.similarity('homen', 'rei')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import DBSCAN, KMeans, MiniBatchKMeans\n",
    "from sklearn.metrics import silhouette_score\n",
    "from scipy.spatial.distance import cdist\n",
    "import matplotlib.pyplot as plt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('../data/processed/words_embedded.pickle', 'rb') as handle:\n",
    "    words_emb = pickle.load(handle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "92921"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(words_emb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([ 0.13466482, -0.33339703,  0.03049505, -0.14442785,  0.4825119 ,\n",
       "         0.09325964, -0.30376583, -0.16224334, -0.3036018 ,  0.3862049 ,\n",
       "        -0.02629075,  0.11636933, -0.09413078,  0.15269203, -0.57769305,\n",
       "         0.13305959,  0.14668672, -0.1670519 , -0.14813097,  0.526141  ,\n",
       "        -0.02352826,  0.01080366, -0.17984863, -0.12925963, -0.1232844 ,\n",
       "         0.11996371, -0.15587   , -0.51917297, -0.22072664, -0.3079447 ,\n",
       "         0.08658405,  0.11462795, -0.20802052, -0.2547001 ,  0.11560781,\n",
       "        -0.3833881 ,  0.29114178,  0.1305361 , -0.01813705,  0.03185189,\n",
       "        -0.102152  ,  0.4646123 ,  0.0557541 ,  0.20979929,  0.03325607,\n",
       "        -0.2373364 ,  0.0556756 ,  0.11174974, -0.55730236, -0.04179006,\n",
       "        -0.00191056, -0.09842405,  0.48407012, -0.06027829, -0.07975583,\n",
       "        -0.11629441,  0.3252713 , -0.26345727,  0.12171944,  0.3118627 ,\n",
       "        -0.27259988, -0.29362404,  0.1995114 ,  0.29705742,  0.07984243,\n",
       "        -0.25047162,  0.65206164, -0.27045733, -0.31018585, -0.16448446,\n",
       "         0.26430985,  0.29580817,  0.05396565, -0.12961027, -0.18328136,\n",
       "         0.0939232 ,  0.3585019 ,  0.03918918,  0.12110697, -0.27290684,\n",
       "         0.41453654,  0.08034559, -0.59585625, -0.0673973 ,  0.27604628,\n",
       "        -0.3738542 , -0.12754856, -0.14934866,  0.14816424,  0.34316698,\n",
       "        -0.4194133 , -0.54708135, -0.28256702,  0.17099911,  0.16726866,\n",
       "        -0.03884285,  0.11206715, -0.33010682, -0.00131153,  0.4299898 ],\n",
       "       dtype=float32),\n",
       " 'aa')"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "words_emb[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(words_emb[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = [val[0] for val in words_emb]\n",
    "ks = range(2, 12)\n",
    "results = {}\n",
    "for k in ks: \n",
    "    kmeans = KMeans(n_clusters=k, random_state=0)\n",
    "    kmeans.fit(X)\n",
    "    results[k] = silhouette_score(X, kmeans.labels_)\n",
    "    print(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([3, 0, 0, ..., 0, 0, 0], dtype=int32)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "kmeans_preds.labels_ "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kmeans = KMeans(n_clusters=10, random_state=0)\n",
    "kmeans.fit(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kmeans = MiniBatchKMeans(n_clusters=20,\n",
    "                         random_state=0,\n",
    "                         batch_size=12)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kmeans.fit(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "distortions = []\n",
    "cluster_numbers = range(2, 20)\n",
    "for k in cluster_numbers:\n",
    "    k_means = KMeans(n_clusters=k, random_state=42)\n",
    "    k_means.fit(X)\n",
    "    distortions.append(sum(np.min(cdist(X, k_means.cluster_centers_, 'euclidean'), axis=1)) / len(X))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_line = [cluster_numbers[0], cluster_numbers[-1]]\n",
    "Y_line = [distortions[0], distortions[-1]]\n",
    "\n",
    "plt.figure(figsize=(10,10))\n",
    "plt.plot(cluster_numbers, distortions, 'b-')\n",
    "plt.plot(X_line, Y_line, 'r')\n",
    "plt.xlabel('k')\n",
    "plt.ylabel('Distortion')\n",
    "plt.title('The Elbow Method showing the optimal k')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "res = pd.DataFrame()\n",
    "res['Word'] = [val[1] for val in words_emb]\n",
    "res['Emb'] = [val[0] for val in words_emb]\n",
    "res['Concept'] = kmeans.labels_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Word</th>\n",
       "      <th>Emb</th>\n",
       "      <th>Concept</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>aa</td>\n",
       "      <td>[0.023016687, 0.084352694, 0.15179141, -0.1909...</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>aaai</td>\n",
       "      <td>[0.0070010833, 0.015572468, 0.030469012, -0.03...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>aaba</td>\n",
       "      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>aabreuexpresso</td>\n",
       "      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>aachoo</td>\n",
       "      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             Word                                                Emb  Concept\n",
       "0              aa  [0.023016687, 0.084352694, 0.15179141, -0.1909...        3\n",
       "1            aaai  [0.0070010833, 0.015572468, 0.030469012, -0.03...        0\n",
       "2            aaba  [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...        0\n",
       "3  aabreuexpresso  [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...        0\n",
       "4          aachoo  [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...        0"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0     65815\n",
       "6      7400\n",
       "11     6261\n",
       "8      4583\n",
       "4      2698\n",
       "3      2532\n",
       "10     2209\n",
       "7      1910\n",
       "9      1818\n",
       "2      1348\n",
       "1       929\n",
       "5       436\n",
       "Name: Concept, dtype: int64"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res.Concept.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res[res.Concept == 8].head(50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim\n",
    "model = gensim.models.Word2Vec.load('../models/trained/word2vec.model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(model.wv.most_similar('furto', topn=10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for concept in np.unique(kmeans_preds.labels_):\n",
    "    print('Concept:', concept)\n",
    "    print(model.wv.most_similar(positive=[np.mean(res[res.Concept==concept]['Emb'])], topn=10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import KeyedVectors\n",
    "model = KeyedVectors.load_word2vec_format('../models/trained/fasttext-sentiment.vec')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('esperansosa', 0.9991340041160583), ('fantasticamente', 0.9990586042404175), ('elisabeth', 0.9987783432006836), ('mtooo', 0.9987491965293884), ('oskar', 0.998711109161377), ('multi', 0.9986676573753357), ('9,0.', 0.9986391067504883), (\"he's\", 0.9986375570297241), ('ame', 0.9986228346824646), ('*-*,', 0.9986050724983215)]\n"
     ]
    }
   ],
   "source": [
    "print(model.most_similar('furto', topn=10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for concept in np.unique(kmeans_preds.labels_):\n",
    "    print('Concept:', concept)\n",
    "    print(model.most_similar(positive=[np.mean(res[res.Concept==concept]['Emb'])], topn=10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning : `load_model` does not return WordVectorModel or SupervisedModel any more, but a `FastText` object which is very similar.\n"
     ]
    }
   ],
   "source": [
    "import fasttext\n",
    "model = fasttext.load_model('../models/trained/fasttext-sentiment.bin')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(0.9591214656829834, 'ouvidas.'), (0.9582967162132263, 'inspirador!'), (0.9571285843849182, 'insano,'), (0.956961989402771, 'albergue...tanto'), (0.9567334651947021, 'filmaco!!!'), (0.956638753414154, 'recomendacao'), (0.9565740823745728, 'velho.'), (0.9564656019210815, '1**'), (0.9563958644866943, 'delicia,'), (0.9563903212547302, '.agora')]\n"
     ]
    }
   ],
   "source": [
    "print(model.get_nearest_neighbors('desporto'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get all words from model\n",
    "words = model.get_words()\n",
    "\n",
    "with open('../models/trained/fasttext-sentiment.vec','w') as file_out:\n",
    "    \n",
    "    # the first line must contain number of total words and vector dimension\n",
    "    file_out.write(str(len(words)) + \" \" + str(model.get_dimension()) + \"\\n\")\n",
    "\n",
    "    # line by line, you append vectors to VEC file\n",
    "    for w in words:\n",
    "        v = model.get_word_vector(w)\n",
    "        vstr = \"\"\n",
    "        for vi in v:\n",
    "            vstr += \" \" + str(vi)\n",
    "        try:\n",
    "            file_out.write(w + vstr+'\\n')\n",
    "        except:\n",
    "            pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for concept in np.unique(kmeans_preds.labels_):\n",
    "    print('Concept:', concept)\n",
    "    print(model.most_similar(positive=[np.mean(res[res.Concept==concept]['Emb'])], topn=10))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sentiment Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from spacytextblob.spacytextblob import SpacyTextBlob\n",
    "\n",
    "nlp = spacy.load(\"pt_core_news_lg\")\n",
    "nlp.add_pipe('spacytextblob')\n",
    "text = 'Este restaurante é uma vergonha!'\n",
    "doc = nlp(text)\n",
    "doc._.blob.polarity                         # Polarity: -0.125\n",
    "doc._.blob.subjectivity                        # Subjectivity: 0.9\n",
    "doc._.blob.sentiment_assessments.assessments   # Assessments: [(['really', 'horrible'], -1.0, 1.0, None), (['worst', '!'], -1.0, 1.0, None), (['really', 'good'], 0.7, 0.6000000000000001, None), (['happy'], 0.8, 1.0, None)]\n",
    "doc._.blob.ngrams()                       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc._.blob.polarity "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "from spacytextblob.spacytextblob import SpacyTextBlob\n",
    "from textblob import TextBlob\n",
    "\n",
    "nlp = spacy.load(\"pt_core_news_lg\")\n",
    "nlp.add_pipe('spacytextblob')\n",
    "text = 'mau'\n",
    "doc = nlp(text)\n",
    "doc._.blob.polarity                            # Polarity: -0.125\n",
    "doc._.blob.subjectivity                        # Subjectivity: 0.9\n",
    "doc._.blob.sentiment_assessments.assessments   # Assessments: [(['really', 'horrible'], -1.0, 1.0, None), (['worst', '!'], -1.0, 1.0, None), (['really', 'good'], 0.7, 0.6000000000000001, None), (['happy'], 0.8, 1.0, None)]\n",
    "#doc._.blob.ngrams()      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc._.blob.sentiment_assessments.assessments"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## External data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ast\n",
    "from sklearn.model_selection import StratifiedShuffleSplit\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_7619/11413983.py:1: DtypeWarning: Columns (1) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df = pd.read_csv('../data/external_corpus/concatenated.csv')\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv('../data/external_corpus/concatenated.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>dataset</th>\n",
       "      <th>original_index</th>\n",
       "      <th>review_text</th>\n",
       "      <th>review_text_processed</th>\n",
       "      <th>review_text_tokenized</th>\n",
       "      <th>polarity</th>\n",
       "      <th>rating</th>\n",
       "      <th>kfold_polarity</th>\n",
       "      <th>kfold_rating</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>b2w</td>\n",
       "      <td>11955</td>\n",
       "      <td>Bem macio e felpudo...recomendo.  Preço imbatí...</td>\n",
       "      <td>bem macio e felpudo...recomendo.  preco imbati...</td>\n",
       "      <td>['bem', 'macio', 'felpudo', 'recomendo', 'prec...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>b2w</td>\n",
       "      <td>35478</td>\n",
       "      <td>Produto excepcional!  recomendo!!! inovador e ...</td>\n",
       "      <td>produto excepcional!  recomendo!!! inovador e ...</td>\n",
       "      <td>['produto', 'excepcional', 'recomendo', 'inova...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>b2w</td>\n",
       "      <td>122760</td>\n",
       "      <td>recebi o produto antes do prazo mas veio com d...</td>\n",
       "      <td>recebi o produto antes do prazo mas veio com d...</td>\n",
       "      <td>['recebi', 'produto', 'antes', 'do', 'prazo', ...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>b2w</td>\n",
       "      <td>17114</td>\n",
       "      <td>Bom custo beneficio. Adequado para pessoas que...</td>\n",
       "      <td>bom custo beneficio. adequado para pessoas que...</td>\n",
       "      <td>['bom', 'custo', 'beneficio', 'adequado', 'par...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>b2w</td>\n",
       "      <td>19112</td>\n",
       "      <td>Além de higiênico tem o tamanho ideal. Só falt...</td>\n",
       "      <td>alem de higienico tem o tamanho ideal. so falt...</td>\n",
       "      <td>['alem', 'de', 'higienico', 'tem', 'tamanho', ...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>3.0</td>\n",
       "      <td>-1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  dataset original_index                                        review_text  \\\n",
       "0     b2w          11955  Bem macio e felpudo...recomendo.  Preço imbatí...   \n",
       "1     b2w          35478  Produto excepcional!  recomendo!!! inovador e ...   \n",
       "2     b2w         122760  recebi o produto antes do prazo mas veio com d...   \n",
       "3     b2w          17114  Bom custo beneficio. Adequado para pessoas que...   \n",
       "4     b2w          19112  Além de higiênico tem o tamanho ideal. Só falt...   \n",
       "\n",
       "                               review_text_processed  \\\n",
       "0  bem macio e felpudo...recomendo.  preco imbati...   \n",
       "1  produto excepcional!  recomendo!!! inovador e ...   \n",
       "2  recebi o produto antes do prazo mas veio com d...   \n",
       "3  bom custo beneficio. adequado para pessoas que...   \n",
       "4  alem de higienico tem o tamanho ideal. so falt...   \n",
       "\n",
       "                               review_text_tokenized  polarity  rating  \\\n",
       "0  ['bem', 'macio', 'felpudo', 'recomendo', 'prec...       1.0     4.0   \n",
       "1  ['produto', 'excepcional', 'recomendo', 'inova...       1.0     5.0   \n",
       "2  ['recebi', 'produto', 'antes', 'do', 'prazo', ...       0.0     1.0   \n",
       "3  ['bom', 'custo', 'beneficio', 'adequado', 'par...       1.0     5.0   \n",
       "4  ['alem', 'de', 'higienico', 'tem', 'tamanho', ...       NaN     3.0   \n",
       "\n",
       "   kfold_polarity  kfold_rating  \n",
       "0               1             1  \n",
       "1               1             1  \n",
       "2               1             1  \n",
       "3               1             1  \n",
       "4              -1             1  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sentiment(val):\n",
    "    if val < 3:\n",
    "        return 'Negative'\n",
    "    elif val == 3:\n",
    "        return 'Neutral'\n",
    "    elif val > 3:\n",
    "        return 'Positive'\n",
    "\n",
    "df['sentiment'] = df['rating'].apply(lambda val : sentiment(val))       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "utlc_movies    1487449\n",
       "utlc_apps      1039535\n",
       "b2w             132373\n",
       "buscape          84991\n",
       "olist            41744\n",
       "Name: dataset, dtype: int64"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['dataset'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Positive    1976535\n",
       "Negative     409629\n",
       "Neutral      399928\n",
       "Name: sentiment, dtype: int64"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['sentiment'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>dataset</th>\n",
       "      <th>original_index</th>\n",
       "      <th>review_text</th>\n",
       "      <th>review_text_processed</th>\n",
       "      <th>review_text_tokenized</th>\n",
       "      <th>polarity</th>\n",
       "      <th>rating</th>\n",
       "      <th>kfold_polarity</th>\n",
       "      <th>kfold_rating</th>\n",
       "      <th>sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>b2w</td>\n",
       "      <td>19112</td>\n",
       "      <td>Além de higiênico tem o tamanho ideal. Só falt...</td>\n",
       "      <td>alem de higienico tem o tamanho ideal. so falt...</td>\n",
       "      <td>['alem', 'de', 'higienico', 'tem', 'tamanho', ...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>3.0</td>\n",
       "      <td>-1</td>\n",
       "      <td>1</td>\n",
       "      <td>Neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>b2w</td>\n",
       "      <td>50688</td>\n",
       "      <td>Ué não entendi! Estava procurando recomendaçõe...</td>\n",
       "      <td>ue nao entendi! estava procurando recomendacoe...</td>\n",
       "      <td>['ue', 'nao', 'entendi', 'estava', 'procurando...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>3.0</td>\n",
       "      <td>-1</td>\n",
       "      <td>1</td>\n",
       "      <td>Neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>b2w</td>\n",
       "      <td>130351</td>\n",
       "      <td>Não chegou ainda então não tem como ser avalia...</td>\n",
       "      <td>nao chegou ainda entao nao tem como ser avalia...</td>\n",
       "      <td>['nao', 'chegou', 'ainda', 'entao', 'nao', 'te...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>3.0</td>\n",
       "      <td>-1</td>\n",
       "      <td>1</td>\n",
       "      <td>Neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>b2w</td>\n",
       "      <td>8590</td>\n",
       "      <td>Qualida do produto dentro das espectativas. En...</td>\n",
       "      <td>qualida do produto dentro das espectativas. en...</td>\n",
       "      <td>['qualida', 'do', 'produto', 'dentro', 'das', ...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>3.0</td>\n",
       "      <td>-1</td>\n",
       "      <td>1</td>\n",
       "      <td>Neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>52</th>\n",
       "      <td>b2w</td>\n",
       "      <td>41736</td>\n",
       "      <td>Bom produto, chegou antes do prazo, fácil de m...</td>\n",
       "      <td>bom produto, chegou antes do prazo, facil de m...</td>\n",
       "      <td>['bom', 'produto', 'chegou', 'antes', 'do', 'p...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>3.0</td>\n",
       "      <td>-1</td>\n",
       "      <td>1</td>\n",
       "      <td>Neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2786063</th>\n",
       "      <td>buscape</td>\n",
       "      <td>0_316604</td>\n",
       "      <td>Não é ruim, mas não me parece tão bom como out...</td>\n",
       "      <td>nao e ruim, mas nao me parece tao bom como out...</td>\n",
       "      <td>['nao', 'ruim', 'mas', 'nao', 'me', 'parece', ...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>3.0</td>\n",
       "      <td>-1</td>\n",
       "      <td>10</td>\n",
       "      <td>Neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2786071</th>\n",
       "      <td>buscape</td>\n",
       "      <td>0_195702</td>\n",
       "      <td>É bonito, barato e parece ser bem feito e func...</td>\n",
       "      <td>e bonito, barato e parece ser bem feito e func...</td>\n",
       "      <td>['bonito', 'barato', 'parece', 'ser', 'bem', '...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>3.0</td>\n",
       "      <td>-1</td>\n",
       "      <td>10</td>\n",
       "      <td>Neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2786080</th>\n",
       "      <td>buscape</td>\n",
       "      <td>2_395354</td>\n",
       "      <td>esta é uma máquina de desempenho muito bom\\n\\n...</td>\n",
       "      <td>esta e uma maquina de desempenho muito bom\\n\\n...</td>\n",
       "      <td>['esta', 'uma', 'maquina', 'de', 'desempenho',...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>3.0</td>\n",
       "      <td>-1</td>\n",
       "      <td>10</td>\n",
       "      <td>Neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2786088</th>\n",
       "      <td>buscape</td>\n",
       "      <td>minus_1_150466</td>\n",
       "      <td>O esquema antigo de desmontagem e limpeza das ...</td>\n",
       "      <td>o esquema antigo de desmontagem e limpeza das ...</td>\n",
       "      <td>['esquema', 'antigo', 'de', 'desmontagem', 'li...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>3.0</td>\n",
       "      <td>-1</td>\n",
       "      <td>10</td>\n",
       "      <td>Neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2786090</th>\n",
       "      <td>buscape</td>\n",
       "      <td>0_389898</td>\n",
       "      <td>Muito bom e intuitivo!\\n\\nO que gostei: Educa ...</td>\n",
       "      <td>muito bom e intuitivo!\\n\\no que gostei: educa ...</td>\n",
       "      <td>['muito', 'bom', 'intuitivo', 'que', 'gostei',...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>3.0</td>\n",
       "      <td>-1</td>\n",
       "      <td>10</td>\n",
       "      <td>Neutral</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>399928 rows × 10 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         dataset  original_index  \\\n",
       "4            b2w           19112   \n",
       "14           b2w           50688   \n",
       "30           b2w          130351   \n",
       "38           b2w            8590   \n",
       "52           b2w           41736   \n",
       "...          ...             ...   \n",
       "2786063  buscape        0_316604   \n",
       "2786071  buscape        0_195702   \n",
       "2786080  buscape        2_395354   \n",
       "2786088  buscape  minus_1_150466   \n",
       "2786090  buscape        0_389898   \n",
       "\n",
       "                                               review_text  \\\n",
       "4        Além de higiênico tem o tamanho ideal. Só falt...   \n",
       "14       Ué não entendi! Estava procurando recomendaçõe...   \n",
       "30       Não chegou ainda então não tem como ser avalia...   \n",
       "38       Qualida do produto dentro das espectativas. En...   \n",
       "52       Bom produto, chegou antes do prazo, fácil de m...   \n",
       "...                                                    ...   \n",
       "2786063  Não é ruim, mas não me parece tão bom como out...   \n",
       "2786071  É bonito, barato e parece ser bem feito e func...   \n",
       "2786080  esta é uma máquina de desempenho muito bom\\n\\n...   \n",
       "2786088  O esquema antigo de desmontagem e limpeza das ...   \n",
       "2786090  Muito bom e intuitivo!\\n\\nO que gostei: Educa ...   \n",
       "\n",
       "                                     review_text_processed  \\\n",
       "4        alem de higienico tem o tamanho ideal. so falt...   \n",
       "14       ue nao entendi! estava procurando recomendacoe...   \n",
       "30       nao chegou ainda entao nao tem como ser avalia...   \n",
       "38       qualida do produto dentro das espectativas. en...   \n",
       "52       bom produto, chegou antes do prazo, facil de m...   \n",
       "...                                                    ...   \n",
       "2786063  nao e ruim, mas nao me parece tao bom como out...   \n",
       "2786071  e bonito, barato e parece ser bem feito e func...   \n",
       "2786080  esta e uma maquina de desempenho muito bom\\n\\n...   \n",
       "2786088  o esquema antigo de desmontagem e limpeza das ...   \n",
       "2786090  muito bom e intuitivo!\\n\\no que gostei: educa ...   \n",
       "\n",
       "                                     review_text_tokenized  polarity  rating  \\\n",
       "4        ['alem', 'de', 'higienico', 'tem', 'tamanho', ...       NaN     3.0   \n",
       "14       ['ue', 'nao', 'entendi', 'estava', 'procurando...       NaN     3.0   \n",
       "30       ['nao', 'chegou', 'ainda', 'entao', 'nao', 'te...       NaN     3.0   \n",
       "38       ['qualida', 'do', 'produto', 'dentro', 'das', ...       NaN     3.0   \n",
       "52       ['bom', 'produto', 'chegou', 'antes', 'do', 'p...       NaN     3.0   \n",
       "...                                                    ...       ...     ...   \n",
       "2786063  ['nao', 'ruim', 'mas', 'nao', 'me', 'parece', ...       NaN     3.0   \n",
       "2786071  ['bonito', 'barato', 'parece', 'ser', 'bem', '...       NaN     3.0   \n",
       "2786080  ['esta', 'uma', 'maquina', 'de', 'desempenho',...       NaN     3.0   \n",
       "2786088  ['esquema', 'antigo', 'de', 'desmontagem', 'li...       NaN     3.0   \n",
       "2786090  ['muito', 'bom', 'intuitivo', 'que', 'gostei',...       NaN     3.0   \n",
       "\n",
       "         kfold_polarity  kfold_rating sentiment  \n",
       "4                    -1             1   Neutral  \n",
       "14                   -1             1   Neutral  \n",
       "30                   -1             1   Neutral  \n",
       "38                   -1             1   Neutral  \n",
       "52                   -1             1   Neutral  \n",
       "...                 ...           ...       ...  \n",
       "2786063              -1            10   Neutral  \n",
       "2786071              -1            10   Neutral  \n",
       "2786080              -1            10   Neutral  \n",
       "2786088              -1            10   Neutral  \n",
       "2786090              -1            10   Neutral  \n",
       "\n",
       "[399928 rows x 10 columns]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[df.sentiment == 'Neutral']['']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df[df.dataset.isin(['olist', 'b2w'])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([4., 5., 1., 3., 2.])"
      ]
     },
     "execution_count": 158,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.rating.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 1.,  0., nan])"
      ]
     },
     "execution_count": 159,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.polarity.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0    106971\n",
       "0.0     47166\n",
       "Name: polarity, dtype: int64"
      ]
     },
     "execution_count": 160,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['polarity'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "174117\n",
      "173841\n"
     ]
    }
   ],
   "source": [
    "reviews = [(ast.literal_eval(doc), r) for doc, r in zip(df['review_text_tokenized'].to_list(), df['sentiment'].tolist())]\n",
    "print(len(reviews))\n",
    "data = [(doc, r) for doc, r in reviews if len(doc) > 0]\n",
    "print(len(data))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.array([' '.join(val[0]) for val in data])\n",
    "Y = np.array([val[1] for val in data])\n",
    "\n",
    "sss = StratifiedShuffleSplit(n_splits=1, test_size=0.1, random_state=0)\n",
    "for train_index, test_index in sss.split(X, Y ):\n",
    "    X_train, X_test = X[train_index], X[test_index]\n",
    "    y_train, y_test = Y[train_index], Y[test_index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array(['Negative', 'Neutral', 'Positive'], dtype='<U8'),\n",
       " array([1139,  365, 2653]))"
      ]
     },
     "execution_count": 124,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.unique(y_test, return_counts=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array(['Negative', 'Neutral', 'Positive'], dtype='<U8'),\n",
       " array([10254,  3286, 23872]))"
      ]
     },
     "execution_count": 125,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.unique(y_train, return_counts=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['avaliacao acima expressa minha opiniao',\n",
       "       'produto venho em otimo estado perfeitamente lacrado chegou bem antes do prazo recomendo',\n",
       "       'interfone nao funciona que eu faco', ..., 'tudo ok',\n",
       "       'entregue antes do prazo estipulado', 'excelente'], dtype='<U206')"
      ]
     },
     "execution_count": 126,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import naive_bayes\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn import linear_model\n",
    "from scipy.sparse import vstack, hstack\n",
    "from sklearn.metrics import classification_report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [],
   "source": [
    "count_vect = TfidfVectorizer()\n",
    "X_train_ = count_vect.fit_transform(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [],
   "source": [
    "#nb=naive_bayes.BernoulliNB()\n",
    "#nb.fit(X_train,y_train)\n",
    "#prob=nb.feature_log_prob_ #index 0 is positive\n",
    "\n",
    "#r=prob[0]-prob[1]\n",
    "\n",
    "#print('Weighing features')\n",
    "#X_train=[x.multiply(r).tocsr() for x in X_train_NB]\n",
    "#X_train=vstack(X_train)\n",
    "\n",
    "X_test_=count_vect.transform(X_test)\n",
    "#X_test=[x.multiply(r).tocsr() for x in X_test_pre]\n",
    "#X_test=vstack(X_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training classifier\n",
      "Testing classifier\n",
      "Accuracy= 84.55617031513111\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    Negative       0.77      0.88      0.82      1139\n",
      "     Neutral       0.32      0.06      0.11       365\n",
      "    Positive       0.89      0.94      0.91      2653\n",
      "\n",
      "    accuracy                           0.85      4157\n",
      "   macro avg       0.66      0.63      0.61      4157\n",
      "weighted avg       0.81      0.85      0.82      4157\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/joao/.cache/pypoetry/virtualenvs/smart_archive-LyPfE8oW-py3.9/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    }
   ],
   "source": [
    "print('Training classifier')\n",
    "\n",
    "svc = linear_model.LogisticRegression()\n",
    "svc.fit(X_train_,y_train)\n",
    "\n",
    "print('Testing classifier')\n",
    "print('Accuracy=', svc.score(X_test_, y_test)*100)\n",
    "print(classification_report(y_test, svc.predict(X_test_)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing classifier\n",
      "Accuracy= 83.32932403175367\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    Negative       0.76      0.89      0.82      1139\n",
      "     Neutral       0.31      0.27      0.29       365\n",
      "    Positive       0.94      0.89      0.91      2653\n",
      "\n",
      "    accuracy                           0.83      4157\n",
      "   macro avg       0.67      0.68      0.67      4157\n",
      "weighted avg       0.83      0.83      0.83      4157\n",
      "\n"
     ]
    }
   ],
   "source": [
    "svc = linear_model.SGDClassifier(loss='hinge', class_weight='balanced')\n",
    "svc.fit(X_train_, y_train)\n",
    "\n",
    "print('Testing classifier')\n",
    "print('Accuracy=',svc.score(X_test_, y_test)*100)\n",
    "print(classification_report(y_test, svc.predict(X_test_)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [],
   "source": [
    "import fasttext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(37412, 2)\n",
      "(4157, 2)\n"
     ]
    }
   ],
   "source": [
    "df_train = pd.DataFrame() \n",
    "df_train['sentiment'] = y_train\n",
    "df_train['review'] = X_train\n",
    "\n",
    "df_train['sentiment'] = df_train['sentiment'].apply(lambda val: f'__label__{val}')\n",
    "\n",
    "np.savetxt('olist-train.txt', df_train.values, fmt = \"%s\")\n",
    "\n",
    "print(df_train.shape)\n",
    "\n",
    "df_test = pd.DataFrame() \n",
    "df_test['sentiment'] = y_test\n",
    "df_test['review'] = X_test\n",
    "\n",
    "df_test['sentiment'] = df_test['sentiment'].apply(lambda val: f'__label__{val}')\n",
    "\n",
    "np.savetxt('olist-test.txt', df_test.values, fmt = \"%s\")\n",
    "\n",
    "print(df_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Read 0M words\n",
      "Number of words:  14831\n",
      "Number of labels: 3\n",
      "Progress: 100.0% words/sec/thread:  688938 lr:  0.000000 avg.loss:  0.188959 ETA:   0h 0m 0s\n"
     ]
    }
   ],
   "source": [
    "model = fasttext.train_supervised(input=\"olist-train.txt\", lr=0.05, epoch=50, wordNgrams=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(37412, 0.9863145514808083, 0.9863145514808083)"
      ]
     },
     "execution_count": 147,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.test(\"olist-train.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4157, 0.8373827279287948, 0.8373827279287948)"
      ]
     },
     "execution_count": 148,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.test(\"olist-test.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test['Pred'] = df_test['review'].apply(lambda val: model.predict(val)[0][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sentiment</th>\n",
       "      <th>review</th>\n",
       "      <th>Pred</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>__label__Negative</td>\n",
       "      <td>targaryen para entrega de bijuterias relogios ...</td>\n",
       "      <td>__label__Positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>__label__Positive</td>\n",
       "      <td>estou satisfeita com servico rapido bem embala...</td>\n",
       "      <td>__label__Positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>__label__Positive</td>\n",
       "      <td>recomendados</td>\n",
       "      <td>__label__Positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>__label__Negative</td>\n",
       "      <td>material de pessima qualidade</td>\n",
       "      <td>__label__Negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>__label__Negative</td>\n",
       "      <td>era pra ter chego ate dia 06 06 2018 ja fui no...</td>\n",
       "      <td>__label__Negative</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           sentiment                                             review  \\\n",
       "0  __label__Negative  targaryen para entrega de bijuterias relogios ...   \n",
       "1  __label__Positive  estou satisfeita com servico rapido bem embala...   \n",
       "2  __label__Positive                                       recomendados   \n",
       "3  __label__Negative                      material de pessima qualidade   \n",
       "4  __label__Negative  era pra ter chego ate dia 06 06 2018 ja fui no...   \n",
       "\n",
       "                Pred  \n",
       "0  __label__Positive  \n",
       "1  __label__Positive  \n",
       "2  __label__Positive  \n",
       "3  __label__Negative  \n",
       "4  __label__Negative  "
      ]
     },
     "execution_count": 150,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   precision    recall  f1-score   support\n",
      "\n",
      "__label__Negative       0.77      0.86      0.81      1139\n",
      " __label__Neutral       0.29      0.14      0.19       365\n",
      "__label__Positive       0.90      0.92      0.91      2653\n",
      "\n",
      "         accuracy                           0.84      4157\n",
      "        macro avg       0.66      0.64      0.64      4157\n",
      "     weighted avg       0.81      0.84      0.82      4157\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(df_test['sentiment'], df_test['Pred']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "5ced3be9d1c63e918c342e50044be0791acd9298dca62153cfbef56e0b5c2b15"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
