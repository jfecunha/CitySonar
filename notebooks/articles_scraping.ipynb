{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import json\n",
    "import requests\n",
    "import time\n",
    "\n",
    "import pandas as pd\n",
    "import joblib\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Query Search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "cities = [\n",
    "    \"Lisboa\",\n",
    "    \"Porto\",\n",
    "    \"Setúbal\",\n",
    "    \"Braga\",\n",
    "    \"Aveiro\",\n",
    "    \"Faro\",\n",
    "    \"Leiria\",\n",
    "    \"Santarém\",\n",
    "    \"Coimbra\",\n",
    "    \"Viseu\",\n",
    "    \"Viana do Castelo\",\n",
    "    \"Vila Real\",\n",
    "    \"Castelo Branco\",\n",
    "    \"Évora\",\n",
    "    \"Beja\",\n",
    "    \"Guarda\",\n",
    "    \"Bragança\",\n",
    "    \"Portalegre\",\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exemplo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = 'Porto'\n",
    "websites = 'observador.pt'\n",
    "max_items = 2000\n",
    "from_ = '2019'\n",
    "to_ = '2021'\n",
    "link = f\"https://arquivo.pt/textsearch?q={query}&siteSearch={websites}&maxItems={max_items}&dedupValue=1&prettyPrint=true&from={from_}&to={to_}\"\n",
    "f = requests.get(link)\n",
    "df = json.loads(f.text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2000"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(df['response_items'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Publico API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "keyword= 'viana do castelo'\n",
    "starting_date = '01-01-2010'\n",
    "ending_date = '31-01-2010'\n",
    "page_number = 0\n",
    "link = f\"https://www.publico.pt/api/list/search/?query={keyword}&start={starting_date}&end={ending_date}&page={page_number}\"\n",
    "f = requests.get(link)\n",
    "df = json.loads(f.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['https://www.publico.pt/2010/01/10/sociedade/noticia/frio-e-neve-deixam-norte-e-centro-do-pais-em-alerta-e-condicionam-transito-1417117',\n",
       " 'https://www.publico.pt/2010/01/30/jornal/viana-elimina-passagens--de-nivel-18696347']"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[f['fullUrl'] for f in df]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['https://www.publico.pt/2010/01/10/sociedade/noticia/frio-e-neve-deixam-norte-e-centro-do-pais-em-alerta-e-condicionam-transito-1417117',\n",
       " 'https://www.publico.pt/2010/01/29/desporto/noticia/tribunal-emitiu-ordem-de-apreensao-de-autocarro-do-sp-braga-com-caracter-de-urgencia-1420490']"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[f['fullUrl'] for f in df]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extração capitais de distrito"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "newspapers = ['expresso.pt' ,'publico.pt', 'jn.pt', 'dn.pt', 'cmjornal.pt', 'sol.sapo.pt', 'visao.sapo.pt', 'jornaldenegocios.pt', 'observador.pt']\n",
    "others = ['turismodeportugal.pt', 'nit.pt']\n",
    "sources = newspapers + others"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "max_items = 2000\n",
    "from_ = 1996\n",
    "to_ = 2023\n",
    "\n",
    "columns=['city', 'title', 'content', 'year', 'tstamp', 'link']\n",
    "\n",
    "data = pd.DataFrame(columns=columns)\n",
    "for city in cities:\n",
    "    for year in range(from_, to_):\n",
    "        \n",
    "        link = f\"https://arquivo.pt/textsearch?q={city}&siteSearch={','.join(sources)} \\\n",
    "            &maxItems={max_items}&dedupValue=1&prettyPrint=true&from={str(year)}&to={str(year+1)}\"\n",
    "        \n",
    "        try:\n",
    "        \n",
    "            r = requests.get(link)\n",
    "            payload = json.loads(r.text)['response_items']\n",
    "        \n",
    "            for idx, article in enumerate(payload):\n",
    "                \n",
    "                time.sleep(1)\n",
    "    \n",
    "                data = pd.concat([data, pd.DataFrame({\n",
    "                    'city': city, 'title': article['title'], 'content': requests.get(article['linkToExtractedText']).text, \n",
    "                    'year': year, 'tstamp': article['tstamp'], 'link': article['linkToArchive']}, index=[idx])\n",
    "                ], ignore_index=True)\n",
    "        \n",
    "        except Exception as e:\n",
    "            #print(e)\n",
    "            data = pd.concat([data, pd.DataFrame({\n",
    "                    'city': city, 'title': None, 'content': None, \n",
    "                    'year': year, 'tstamp': None, 'link': None\n",
    "                    }, index=[idx])\n",
    "            ], ignore_index=True)\n",
    "\n",
    "data.to_csv('../data/scraping_data.csv.gz', compression='gzip')   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>city</th>\n",
       "      <th>title</th>\n",
       "      <th>content</th>\n",
       "      <th>year</th>\n",
       "      <th>tstamp</th>\n",
       "      <th>link</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Lisboa</td>\n",
       "      <td>POL | Local Lisboa</td>\n",
       "      <td>POL | Local Lisboa SECÇÕES 1ª Página Destaque ...</td>\n",
       "      <td>1999</td>\n",
       "      <td>19991111042737</td>\n",
       "      <td>https://arquivo.pt/wayback/19991111042737/http...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Lisboa</td>\n",
       "      <td>JN Editorial - Text57</td>\n",
       "      <td>JN Editorial - Text57 26 milhões para dar casa...</td>\n",
       "      <td>1999</td>\n",
       "      <td>19990822002536</td>\n",
       "      <td>https://arquivo.pt/wayback/19990822002536/http...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Lisboa</td>\n",
       "      <td>Outras Paginas</td>\n",
       "      <td>Outras Paginas 11 de Novembro de 1999 Igreja d...</td>\n",
       "      <td>1999</td>\n",
       "      <td>19991117215651</td>\n",
       "      <td>https://arquivo.pt/wayback/19991117215651/http...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Lisboa</td>\n",
       "      <td>JN Editorial - Texult1</td>\n",
       "      <td>JN Editorial - Texult1 Macau: Rão Kyao é o aut...</td>\n",
       "      <td>1999</td>\n",
       "      <td>19991118004529</td>\n",
       "      <td>https://arquivo.pt/wayback/19991118004529/http...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Lisboa</td>\n",
       "      <td>PÚBLICONLINE-Os Destaques da Primeira Página</td>\n",
       "      <td>PÚBLICONLINE-Os Destaques da Primeira Página S...</td>\n",
       "      <td>1999</td>\n",
       "      <td>19991012235908</td>\n",
       "      <td>https://arquivo.pt/wayback/19991012235908/http...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     city                                         title  \\\n",
       "0  Lisboa                            POL | Local Lisboa   \n",
       "1  Lisboa                         JN Editorial - Text57   \n",
       "2  Lisboa                                Outras Paginas   \n",
       "3  Lisboa                        JN Editorial - Texult1   \n",
       "4  Lisboa  PÚBLICONLINE-Os Destaques da Primeira Página   \n",
       "\n",
       "                                             content  year          tstamp  \\\n",
       "0  POL | Local Lisboa SECÇÕES 1ª Página Destaque ...  1999  19991111042737   \n",
       "1  JN Editorial - Text57 26 milhões para dar casa...  1999  19990822002536   \n",
       "2  Outras Paginas 11 de Novembro de 1999 Igreja d...  1999  19991117215651   \n",
       "3  JN Editorial - Texult1 Macau: Rão Kyao é o aut...  1999  19991118004529   \n",
       "4  PÚBLICONLINE-Os Destaques da Primeira Página S...  1999  19991012235908   \n",
       "\n",
       "                                                link  \n",
       "0  https://arquivo.pt/wayback/19991111042737/http...  \n",
       "1  https://arquivo.pt/wayback/19990822002536/http...  \n",
       "2  https://arquivo.pt/wayback/19991117215651/http...  \n",
       "3  https://arquivo.pt/wayback/19991118004529/http...  \n",
       "4  https://arquivo.pt/wayback/19991012235908/http...  "
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Parallelism"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def article_scrapper(city, sources):\n",
    "\n",
    "    max_items = 10\n",
    "    from_ = 1996\n",
    "    to_ = 2023\n",
    "\n",
    "    columns=['city', 'title', 'content', 'year', 'tstamp', 'link']\n",
    "    data = pd.DataFrame(columns=columns)\n",
    "    for year in range(from_, to_):\n",
    "        \n",
    "        link = f\"https://arquivo.pt/textsearch?q={city}&siteSearch={','.join(sources)} \\\n",
    "            &maxItems={max_items}&dedupValue=1&prettyPrint=true&from={str(year)}&to={str(year+1)}\"\n",
    "        \n",
    "        try:\n",
    "        \n",
    "            r = requests.get(link)\n",
    "            payload = json.loads(r.text)['response_items']\n",
    "        \n",
    "            for idx, article in enumerate(payload):\n",
    "                \n",
    "                time.sleep(1)\n",
    "\n",
    "                data = pd.concat([data, pd.DataFrame({\n",
    "                    'city': city, 'title': article['title'], 'content': requests.get(article['linkToExtractedText']).text, \n",
    "                    'year': year, 'tstamp': article['tstamp'], 'link': article['linkToArchive']}, index=[idx])\n",
    "                ], ignore_index=True)\n",
    "        \n",
    "        except Exception as e:\n",
    "            \n",
    "            data = pd.concat([data, pd.DataFrame({\n",
    "                    'city': city, 'title': None, 'content': None, \n",
    "                    'year': year, 'tstamp': None, 'link': None\n",
    "                    }, index=[idx])\n",
    "            ], ignore_index=True)\n",
    "\n",
    "    data.to_csv(f'../data/scraping_data.csv', index=False)   \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "joblib.Parallel(n_jobs=-1)(\n",
    "        joblib.delayed(article_scrapper)(\n",
    "            city=city,\n",
    "            sources=sources,    \n",
    "        )\n",
    "        for city in cities\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "57e5980710ed742cfe72a4be488a9d6f7ea4c6a6f4082a5d701045bcd350c891"
  },
  "kernelspec": {
   "display_name": "Python 3.9.7 ('smart_archive-LyPfE8oW-py3.9')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
