{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Publico scraper"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import sys\n",
    "\n",
    "parent = Path().absolute().parents[0].as_posix()\n",
    "\n",
    "sys.path.insert(0, parent)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import spacy\n",
    "\n",
    "from nlpiper.core import Compose\n",
    "from nlpiper.transformers import cleaners\n",
    "from nlpiper.core import Document\n",
    "\n",
    "from src.cleaners import TextCleaner\n",
    "from resources.stopwords import WORDS\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d_ = pd.read_csv('../data/raw/publico_scraper.csv.gz', compression='gzip')\n",
    "d_.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d_.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d_.dropna(inplace=True)\n",
    "d_.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d_.main_tag.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d_.city.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d_.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d_.tag.value_counts().reset_index(name='N').head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = d_.groupby(['main_tag', 'tag']).size().reset_index(name='N')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[df['main_tag'] == 'edicoes'].sort_values(by=['N'], ascending=False).head(50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tags = ['sociedade', 'local', 'fugas', 'politica', 'desporto', 'p3', 'culturaipsilon', 'economia', 'ciencia', 'opiniao', 'tecnologia', 'ecosfera']\n",
    "\n",
    "tags_to_split = ['sociedade', 'local', 'p3']\n",
    "\n",
    "tags_rename = {'fugas': 'Turismo/Lazer', \n",
    "    'desporto': 'Desporto',\n",
    "    'politica': 'Politica',\n",
    "    'economia': 'Economia',\n",
    "    'ciencia': 'Ciencia',\n",
    "    'culturaipsilon': 'Cultura',\n",
    "    'tecnologia' : 'Tecnologia',\n",
    "    'ecosfera': 'Ambiente',\n",
    "    'opiniao': 'Opiniao'\n",
    "}\n",
    "\n",
    "sub_tags = {\n",
    "    'Saude': ['SAÚDE', 'CORONAVÍRUS', 'COVID-19', 'HOSPITAIS', 'SERVIÇO NACIONAL DE SAÚDE', 'NATALIDADE', 'ASAE', 'INEM'],\n",
    "    'Ambiente': ['METEOROLOGIA', 'MAU TEMPO', 'CLIMA', 'IPMA', 'AMBIENTE', 'ÁGUA', 'SUSTENTABILIDADE', 'FLORESTAS'],\n",
    "    'Incendios': ['INCÊNDIOS', 'INCÊNDIO','INCÊNDIOS FLORESTAIS'],\n",
    "    'Forças-Segurança': ['GNR', 'POLÍCIA JUDICIÁRIA', 'PROTECÇÃO CIVIL', 'PSP', 'SERVIÇO DE ESTRANGEIROS E FRONTEIRAS', 'BOMBEIROS'],\n",
    "    'Educacao': ['ENSINO SUPERIOR', 'EDUCAÇÃO'],\n",
    "    'Justica' : ['JUSTIÇA', 'MINISTÉRIO PÚBLICO'],\n",
    "    'Religiao': ['IGREJA CATÓLICA', 'RELIGIÃO'],\n",
    "    'Crime': ['CRIME', 'VIOLÊNCIA DOMÉSTICA', 'PRISÕES', 'TRÁFICO DE SERES HUMANOS', 'TRÁFICO DE DROGA'],\n",
    "    'Acidentes' : ['ACIDENTES', 'SEGURANÇA RODOVIÁRIA', 'ESTRADAS'],\n",
    "    'Transportes' : ['TRANSPORTES', 'COMBOIOS', 'MOBILIDADE', 'BICICLETAS', 'CP', 'MOBILIDADE', 'AVIAÇÃO'],\n",
    "    'Local' : [\n",
    "        'LISBOA', 'COIMBRA', 'AVEIRO', 'SETÚBAL', 'VIANA DO CASTELO', 'PORTO', 'BRAGANÇA', 'BRAGA', 'BEJA', 'VISEU', 'ÉVORA',\n",
    "        'CÂMARA DE VISEU', 'CÂMARA DE LISBOA', 'VILA REAL', 'ALGARVE', 'LEIRIA', 'CASTELO BRANCO', 'FARO', 'SANTARÉM', 'ALENTEJO',\n",
    "        'CÂMARA DO PORTO', 'PORTALEGRE', 'CÂMARA DE COIMBRA', 'CÂMARA DE BRAGA', 'GUARDA', 'AUTARQUIAS'\n",
    "    ],\n",
    "    'Habitacao' : ['HABITAÇÃO', 'PATRIMÓNIO'],\n",
    "    'Cultura': ['ARTES', 'MÚSICA', 'CULTURA', 'FESTIVAL', 'EVENTO', 'ARTE URBANA', 'STREET ART', 'TEATRO', 'MUSEUS'],\n",
    "    'Tecnologia': ['TECNOLOGIA'],\n",
    "    'Opiniao': ['OPINIÃO', 'REPORTAGEM'],\n",
    "    'Turismo/Lazer': ['TURISMO'],\n",
    "    'Sociedade': ['CRIANÇAS', 'SOLIDARIEDADE', 'CIGANOS', 'SEGURANÇA SOCIAL', 'IDOSOS', 'ANIMAIS']\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_ = d_[d_['main_tag'].isin(tags)]\n",
    "data_.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_to_split = data_[data_['main_tag'].isin(tags_to_split)]\n",
    "print(data_to_split.shape)\n",
    "data_not_to_split = data_[~data_['main_tag'].isin(tags_to_split)]\n",
    "print(data_not_to_split.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def assign_category(val, sub_tags):\n",
    "    for key, values in sub_tags.items():\n",
    "        if val in values:\n",
    "            return(key)\n",
    "   \n",
    "    return 'Outros'\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_to_split['category'] = data_to_split['tag'].apply(lambda val: assign_category(val, sub_tags))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_to_split['category'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply Renaming\n",
    "data_not_to_split['category'] = data_not_to_split['main_tag'].apply(lambda val: tags_rename[val])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_not_to_split['category'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.concat([data_not_to_split, data_to_split], ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.category.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scripts.data_prep import process_stop_words, apply_cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model= spacy.load(\"pt_core_news_lg\")\n",
    "\n",
    "pipeline = Compose([\n",
    "        cleaners.CleanURL(),\n",
    "        cleaners.CleanEOF(),\n",
    "        cleaners.CleanMarkup(),\n",
    "        cleaners.CleanAccents(),\n",
    "        cleaners.CleanNumber()\n",
    "    ])\n",
    "\n",
    "tc = TextCleaner(model=model, stop_words=process_stop_words(pipeline))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from joblib import Parallel, delayed \n",
    "# docs = Parallel(n_jobs=2)(delayed(apply_cleaning)(doc, pipeline, tc) for doc in df['title'].to_list()) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df[~df['category'].isin(['Opiniao', 'Outros'])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.category.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "data_p = pd.DataFrame()\n",
    "data_p['body'] = [apply_cleaning(doc, pipeline, tc) for doc in tqdm(df['body'].to_list())]\n",
    "data_p['title'] = [apply_cleaning(doc, pipeline, tc) for doc in tqdm(df['title'].to_list())]\n",
    "data_p.to_csv('../data/raw/publico_data_processed.csv.gz', compression='gzip', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_p = pd.read_csv('../data/raw/publico_data_processed.csv.gz', compression='gzip')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.array(data_p['body'].to_list())\n",
    "Y = np.array(df['category'].tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn import linear_model\n",
    "from sklearn.metrics import classification_report, f1_score\n",
    "from sklearn.model_selection import StratifiedShuffleSplit\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "score = []\n",
    "sss = StratifiedShuffleSplit(n_splits=2, test_size=0.2, random_state=0)\n",
    "for train_index, test_index in sss.split(X, Y ):\n",
    "    X_train, X_test = X[train_index], X[test_index]\n",
    "    y_train, y_test = Y[train_index], Y[test_index]\n",
    "\n",
    "    vectorizer = TfidfVectorizer()\n",
    "    X_train_ = vectorizer.fit_transform(X_train)\n",
    "    X_test_ = vectorizer.transform(X_test)    \n",
    "\n",
    "    print('Training classifier')\n",
    "    svc = linear_model.LogisticRegression()\n",
    "    svc.fit(X_train_,y_train)\n",
    "\n",
    "    print('Testing classifier')\n",
    "    y_pred = svc.predict(X_test_)\n",
    "    print(classification_report(y_test, y_pred))\n",
    "    score.append(f1_score(y_test, y_pred, average=None))\n",
    "\n",
    "print(np.mean(score))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "score = []\n",
    "sss = StratifiedShuffleSplit(n_splits=2, test_size=0.2, random_state=0)\n",
    "for train_index, test_index in sss.split(X, Y ):\n",
    "    X_train, X_test = X[train_index], X[test_index]\n",
    "    y_train, y_test = Y[train_index], Y[test_index]\n",
    "\n",
    "    vectorizer = TfidfVectorizer()\n",
    "    X_train_ = vectorizer.fit_transform(X_train)\n",
    "    X_test_ = vectorizer.transform(X_test)    \n",
    "\n",
    "    print('Training classifier')\n",
    "    svc = linear_model.SGDClassifier(loss='perceptron', class_weight='balanced')\n",
    "    svc.fit(X_train_, y_train)\n",
    "\n",
    "    print('Testing classifier')\n",
    "    y_pred = svc.predict(X_test_)\n",
    "    print(classification_report(y_test, y_pred))\n",
    "    score.append(f1_score(y_test, y_pred, average='macro'))\n",
    "\n",
    "print(np.mean(score))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import ExtraTreesClassifier\n",
    "\n",
    "score = []\n",
    "sss = StratifiedShuffleSplit(n_splits=2, test_size=0.2, random_state=0)\n",
    "for train_index, test_index in sss.split(X, Y ):\n",
    "    X_train, X_test = X[train_index], X[test_index]\n",
    "    y_train, y_test = Y[train_index], Y[test_index]\n",
    "\n",
    "    vectorizer = TfidfVectorizer()\n",
    "    X_train_ = vectorizer.fit_transform(X_train)\n",
    "    X_test_ = vectorizer.transform(X_test)    \n",
    "\n",
    "    print('Training classifier')\n",
    "    clf = ExtraTreesClassifier(n_estimators=100, random_state=0)\n",
    "    clf.fit(X_train_, y_train)\n",
    "\n",
    "    print('Testing classifier')\n",
    "    y_pred = clf.predict(X_test_)\n",
    "    print(classification_report(y_test, y_pred))\n",
    "    score.append(f1_score(y_test, y_pred, average='macro'))\n",
    "\n",
    "print(np.mean(score))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train = pd.DataFrame() \n",
    "df_train['category'] = y_train\n",
    "df_train['title'] = X_train\n",
    "df_train['category'] = df_train['category'].apply(lambda val: f'__label__{val}')\n",
    "\n",
    "np.savetxt('categories-train.txt', df_train.values, fmt = \"%s\")\n",
    "\n",
    "print(df_train.shape)\n",
    "\n",
    "df_test = pd.DataFrame() \n",
    "df_test['category'] = y_test\n",
    "df_test['title'] = X_test\n",
    "df_test['category'] = df_test['category'].apply(lambda val: f'__label__{val}')\n",
    "\n",
    "print(df_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import fasttext\n",
    "model = fasttext.train_supervised(input=\"categories-train.txt\", lr=1, epoch=100, wordNgrams=5)\n",
    "df_test['Pred'] = df_test['title'].apply(lambda val: model.predict(val)[0][0])\n",
    "print(classification_report(df_test['category'], df_test['Pred']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Neural Net (RNN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.utils import to_categorical\n",
    "from sklearn import preprocessing\n",
    "\n",
    "le = preprocessing.LabelEncoder()\n",
    "le.fit(y_train)\n",
    "\n",
    "n_classes = len(np.unique(y_train))\n",
    "\n",
    "train_dataset = tf.data.Dataset.from_tensor_slices((X_train, to_categorical(le.transform(y_train), num_classes=n_classes)))\n",
    "test_dataset = tf.data.Dataset.from_tensor_slices((X_test, to_categorical(le.transform(y_test), num_classes=n_classes)))\n",
    "\n",
    "BATCH_SIZE = 64\n",
    "SHUFFLE_BUFFER_SIZE = 100\n",
    "\n",
    "train_dataset = train_dataset.shuffle(SHUFFLE_BUFFER_SIZE).batch(BATCH_SIZE)\n",
    "test_dataset = test_dataset.batch(BATCH_SIZE)\n",
    "\n",
    "\n",
    "VOCAB_SIZE = 100000\n",
    "encoder = tf.keras.layers.TextVectorization(\n",
    "    max_tokens=VOCAB_SIZE)\n",
    "encoder.adapt(train_dataset.map(lambda text, label: text))\n",
    "\n",
    "\n",
    "model = tf.keras.Sequential([\n",
    "    encoder,\n",
    "    tf.keras.layers.Embedding(\n",
    "        input_dim=len(encoder.get_vocabulary()),\n",
    "        output_dim=100,\n",
    "        # Use masking to handle the variable sequence lengths\n",
    "        mask_zero=True),\n",
    "    tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(100)),\n",
    "    tf.keras.layers.Dense(64, activation='relu'),\n",
    "    tf.keras.layers.Dense(n_classes, activation='softmax')\n",
    "])\n",
    "callback = tf.keras.callbacks.EarlyStopping(\n",
    "    monitor='val_loss', \n",
    "    patience=3, \n",
    "    restore_best_weights=True, \n",
    "    mode='min'\n",
    ")\n",
    "\n",
    "# Hyperparameters\n",
    "epochs = 10\n",
    "learning_rate = 0.001\n",
    "decay_rate = learning_rate / epochs\n",
    "\n",
    "opt = tf.keras.optimizers.Adam(\n",
    "    learning_rate=learning_rate,\n",
    "    #decay=decay_rate \n",
    "    )\n",
    "\n",
    "model.compile(\n",
    "    optimizer=opt, \n",
    "    loss=tf.keras.losses.CategoricalCrossentropy(),\n",
    "    metrics=[tf.metrics.Precision()]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "model.fit(\n",
    "    train_dataset,\n",
    "    validation_data=test_dataset,\n",
    "    epochs=epochs,\n",
    "    callbacks=callback\n",
    " )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.evaluate(test_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds = model.predict(test_dataset)\n",
    "preds = [le.inverse_transform([np.argmax(pred)])[0] for pred in preds]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(classification_report(y_test, preds))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sentiment\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = fasttext.load_model('../models/trained/fasttext-sentiment.bin')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_p"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_p['sentiment'] = data['']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.predict(data_p['title'].iloc[0])[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sent = []\n",
    "indexes = []\n",
    "for i, val in enumerate(data_p['title'].to_list()):\n",
    "    try:\n",
    "        sent.append(model.predict(val))\n",
    "        indexes.append(i)\n",
    "    except:\n",
    "        continue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_p.iloc[indexes]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = data_p.iloc[indexes]\n",
    "results['sentiment'] = [val[0][0] for val in sent]\n",
    "results['score'] = [val[1][0] for val in sent]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results[['title', 'sentiment']].head(50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results.score.hist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results[(results.sentiment == '__label__Negative') & (results.score > 0.80)].head(50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "aspects = []\n",
    "nlp= spacy.load(\"pt_core_news_lg\")\n",
    "\n",
    "for sentence in d_.iloc[-1].body.split('.'):\n",
    "  doc = nlp(sentence)\n",
    "  descriptive_term = ''\n",
    "  target = ''\n",
    "  for token in doc:\n",
    "    if token.dep_ == 'nsubj' and token.pos_ == 'NOUN':\n",
    "      target = token.text\n",
    "    if token.pos_ == 'ADJ':\n",
    "      prepend = ''\n",
    "      for child in token.children:\n",
    "        if child.pos_ != 'ADV':\n",
    "          continue\n",
    "        prepend += child.text + ' '\n",
    "      descriptive_term = prepend + token.text\n",
    "  if target and descriptive_term:    \n",
    "    aspects.append({'aspect': target,\n",
    "      'description': descriptive_term, 'sentence': sentence})\n",
    "print(aspects)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for sentence in d_.iloc[-1].body.split('.'):\n",
    "  doc = nlp(sentence)\n",
    "  for token in doc:\n",
    "    print(token.text, token.dep_, token.head.text, token.head.pos_,\n",
    "      token.pos_,[child for child in token.children])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for sentence in d_.iloc[-3].body.split('.'):\n",
    "  doc = nlp(sentence)\n",
    "  descriptive_term = ''\n",
    "  for token in doc:\n",
    "    if token.pos_ == 'ADJ':\n",
    "      descriptive_term = token\n",
    "  #print(sentence)\n",
    "  print(descriptive_term)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for sentence in d_.iloc[0].body.split('.'):\n",
    "  doc = nlp(sentence)\n",
    "  descriptive_term = ''\n",
    "  for token in doc:\n",
    "    if token.pos_ == 'ADJ':\n",
    "      prepend = ''\n",
    "      for child in token.children:\n",
    "        if child.pos_ != 'ADV':\n",
    "          continue\n",
    "        prepend += child.text + ' '\n",
    "      descriptive_term = prepend + token.text\n",
    "  print(sentence)\n",
    "  print(descriptive_term)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d_.iloc[0].body.split('.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "57e5980710ed742cfe72a4be488a9d6f7ea4c6a6f4082a5d701045bcd350c891"
  },
  "kernelspec": {
   "display_name": "Python 3.9.7 ('smart_archive-LyPfE8oW-py3.9')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
